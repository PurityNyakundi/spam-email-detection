#begin by impoting the dataset
sms_raw <- read.csv("spam.csv", stringsAsFactors = FALSE)
#check the structure
str(sms_raw)
#change the type or v1 into caartegorical variable
sms_raw$v1<-as.factor(sms_raw$v1)
#study the target variable
table(sms_raw$v1)
#use the text mining package to preprocess data for analysis
library(tm)
#first create a corpus for each single string
sms_corpus <- VCorpus(VectorSource(sms_raw$v2))
sms_corpus
lapply(sms_corpus[4:7], as.character)
#lets check what was created
inspect(sms_corpus[1:3])
head(inspect(sms_corpus))
#next we do text cleaning
#convert each text to lower for uniformity so that "Word" will not be different as "word"
corpus_clean <- tm_map(sms_corpus, content_transformer(tolower))
#next we remove numbers
corpus_clean <- tm_map(corpus_clean, removeNumbers)
inspect(corpus_clean)
lapply(corpus_clean[4:7], as.character)
#next we remove stop words such as but or
corpus_clean <- tm_map(corpus_clean, removeWords, stopwords())
#next we remove punctuation
corpus_clean <- tm_map(corpus_clean, removePunctuation)
#we remove white spaces and leave the single whitespace
corpus_clean <- tm_map(corpus_clean, stripWhitespace)

#next we split strings to sigle words called tokens
sms_dtm <- DocumentTermMatrix(corpus_clean)
sms_dtm

sms_dtm2 <- DocumentTermMatrix(sms_corpus, control = list(
  tolower = TRUE,
  removeNumbers = TRUE,
  stopwords = TRUE,
  removePunctuation = TRUE,
  stemming = TRUE
))
sms_dtm2

#next we do data preparation spliting the data into train and test
#first split raw data
sms_raw_train <- sms_raw[1:4169, ]
sms_raw_test <- sms_raw[4170:5559, ]
#Then the document-term matrix:
 sms_dtm_train <- sms_dtm2[1:4169, ]
sms_dtm_test <- sms_dtm2[4170:5559, ]
#And finally, the corpus:
sms_corpus_train <- corpus_clean[1:4169]
sms_corpus_test <- corpus_clean[4170:5559]



#next visualize the data
#we use a wordcloud
library(wordcloud)
wordcloud(sms_corpus_train, min.freq = 40, random.order= F)


spam <- subset(sms_raw_train, v1 == "spam")
#Next, we'll do the same thing for the ham subset:
ham <- subset(sms_raw_train, v1 == "ham")
head(spam)
wordcloud(spam$v2, max.words = 40, scale = c(3, 0.5))
wordcloud(ham$v2, max.words = 40, scale = c(3, 0.5))

#create frequent occuring words
myterms<-as.vector(findFreqTerms(sms_dtm_train, 5))
head(myterms)
#store most occuring words in a dictonary
library(tidyverse)
#sms_dict <- Dictionary(findFreqTerms(sms_dtm_train, 5))
sms_train <- DocumentTermMatrix(sms_corpus_train,
                                list(dictionary = myterms))
sms_test <- DocumentTermMatrix(sms_corpus_test,list(dictionary = myterms))

convert_counts <- function(x) {
  x <- ifelse(x > 0, "Yes", "No")
}
sms_train <- apply(sms_train, MARGIN = 2, convert_counts)
sms_test <- apply(sms_test, MARGIN = 2, convert_counts)
library(e1071)
sms_classifier <- naiveBayes(sms_train, sms_raw_train$v1)
sms_test_pred <- predict(sms_classifier, sms_test)
library(gmodels)
CrossTable(sms_test_pred, sms_raw_test$v1,
             prop.chisq = FALSE, prop.t = FALSE,
             dnn = c('predicted', 'actual'))
